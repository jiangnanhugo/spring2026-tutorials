{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25874688",
   "metadata": {},
   "source": [
    "# Homework 1 (Programming): From Linear Regression (Scratch) to a 3-Layer MLP (Scratch)\n",
    "\n",
    "\n",
    "\n",
    "- Plotting: `matplotlib`\n",
    "- You may use GPU if available.\n",
    "\n",
    "1. Completed TODOs (code).\n",
    "2. Plots + short written answers in the designated markdown cells.\n",
    "3. export notebook as `lastname_firstname_hw_mlp.pdf` and submit to gradescope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be2d0a",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Run the following cell first. It defines imports and small utilities used throughout the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34765d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, Tuple, Callable, Optional, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# === Reproducibility ===\n",
    "def set_seed(seed: int = 0):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# === Plot helper ===\n",
    "def plot_curves(curves: Dict[str, List[float]], title: str = \"\", xlabel: str = \"step\", ylabel: str = \"value\"):\n",
    "    plt.figure()\n",
    "    for k, v in curves.items():\n",
    "        plt.plot(v, label=k)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e126ef",
   "metadata": {},
   "source": [
    "## 1. Data: Synthetic Regression + Classification\n",
    "\n",
    "You will use the following datasets to test your models. Feel free to modify the data functions **only if you clearly explain why**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression batch shapes: torch.Size([64, 3]) torch.Size([64, 1])\n",
      "Classification batch shapes: torch.Size([64, 3]) torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "class SyntheticRegression(Dataset):\n",
    "    '''\n",
    "    Nonlinear regression dataset:\n",
    "        y = sin(x0) + x1^2 - 0.5*x2 + noise\n",
    "    '''\n",
    "    def __init__(self, n=4000, d=3, noise_std=0.1):\n",
    "        assert d >= 3, \"d must be >= 3 for this formula\"\n",
    "        X = torch.randn(n, d)\n",
    "        y = torch.sin(X[:, 0]) + (X[:, 1] ** 2) - 0.5 * X[:, 2]\n",
    "        y = y + noise_std * torch.randn_like(y)\n",
    "        self.X = X.float()\n",
    "        self.y = y.float().reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "class SyntheticBinaryClassification(Dataset):\n",
    "    '''\n",
    "    Nonlinear binary classification dataset:\n",
    "        logits = sin(x0) + x1^2 - 0.25*x2 + noise\n",
    "        y = 1[logits > 0]\n",
    "    '''\n",
    "    def __init__(self, n=4000, d=3, noise_std=0.2):\n",
    "        assert d >= 3, \"d must be >= 3 for this formula\"\n",
    "        X = torch.randn(n, d)\n",
    "        logits = torch.sin(X[:, 0]) + (X[:, 1] ** 2) - 0.25 * X[:, 2]\n",
    "        logits = logits + noise_std * torch.randn_like(logits)\n",
    "        y = (logits > 0).float().reshape(-1, 1)\n",
    "        self.X = X.float()\n",
    "        self.y = y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "def make_loaders(dataset: Dataset, batch_size=64, val_ratio=0.2, shuffle=True):\n",
    "    n = len(dataset)\n",
    "    n_val = int(n * val_ratio)\n",
    "    n_train = n - n_val\n",
    "    train_ds, val_ds = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(0),\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# quick sanity check\n",
    "train_loader, val_loader = make_loaders(SyntheticRegression(n=512, d=3), batch_size=64)\n",
    "Xb, yb = next(iter(train_loader))\n",
    "print(\"Regression batch shapes:\", Xb.shape, yb.shape)\n",
    "\n",
    "train_loader, val_loader = make_loaders(SyntheticBinaryClassification(n=512, d=3), batch_size=64)\n",
    "Xb, yb = next(iter(train_loader))\n",
    "print(\"Classification batch shapes:\", Xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb65341",
   "metadata": {},
   "source": [
    "# Part 1 — Refactor Training into Reusable Functions (10 pts)\n",
    "\n",
    "### Task\n",
    "Implement:\n",
    "- `train_epoch(model, dataloader, loss_fn, updater)`\n",
    "- `evaluate(model, dataloader, loss_fn, metric_fn=None)`\n",
    "- `fit(...)` that calls the above and returns histories\n",
    "\n",
    "**Required:** keep `loss_history` as **per-step** loss (like your linear regression notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 1 TODO =====\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: Callable, dataloader: DataLoader, loss_fn: Callable, metric_fn: Optional[Callable] = None):\n",
    "    '''\n",
    "    Returns:\n",
    "        avg_loss: float\n",
    "        avg_metric: float or None\n",
    "    '''\n",
    "    model_mode = getattr(model, \"train_mode\", None)\n",
    "    if callable(model_mode):\n",
    "        model.train_mode(False)\n",
    "\n",
    "    total_loss, total_metric, n = 0.0, 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        yhat = model(X)\n",
    "        loss = loss_fn(yhat, y).item()\n",
    "        bs = X.shape[0]\n",
    "        total_loss += loss * bs\n",
    "        n += bs\n",
    "        if metric_fn is not None:\n",
    "            total_metric += metric_fn(yhat, y) * bs\n",
    "\n",
    "    avg_loss = total_loss / max(n, 1)\n",
    "    avg_metric = (total_metric / max(n, 1)) if metric_fn is not None else None\n",
    "    return avg_loss, avg_metric\n",
    "\n",
    "\n",
    "def train_epoch(model: Callable, dataloader: DataLoader, loss_fn: Callable, updater: Callable, log_every: int = 50):\n",
    "    '''\n",
    "    Runs one training epoch.\n",
    "    Returns:\n",
    "        loss_history_step: list of float (per step)\n",
    "    '''\n",
    "    model_mode = getattr(model, \"train_mode\", None)\n",
    "    if callable(model_mode):\n",
    "        model.train_mode(True)\n",
    "\n",
    "    loss_history = []\n",
    "    for step, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        yhat = model(X)\n",
    "        loss = loss_fn(yhat, y)\n",
    "\n",
    "        # TODO: zero grads\n",
    "        # TODO: backward\n",
    "        # TODO: updater()\n",
    "\n",
    "        raise NotImplementedError(\"Implement the training step (zero_grad/backward/update).\")\n",
    "\n",
    "        # loss_history.append(loss.item())\n",
    "        # if (step + 1) % log_every == 0:\n",
    "        #     print(f\"  step {step+1:4d} | loss {loss_history[-1]:.4f}\")\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: Callable,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    loss_fn: Callable,\n",
    "    updater: Callable,\n",
    "    epochs: int = 10,\n",
    "    metric_fn: Optional[Callable] = None,\n",
    "):\n",
    "    '''\n",
    "    Returns:\n",
    "        history dict with keys: train_loss_step, train_loss_epoch, val_loss_epoch, val_metric_epoch\n",
    "    '''\n",
    "    train_loss_step = []\n",
    "    train_loss_epoch = []\n",
    "    val_loss_epoch = []\n",
    "    val_metric_epoch = []\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        step_losses = train_epoch(model, train_loader, loss_fn, updater)\n",
    "        train_loss_step.extend(step_losses)\n",
    "        train_loss_epoch.append(sum(step_losses) / max(len(step_losses), 1))\n",
    "\n",
    "        vloss, vmetric = evaluate(model, val_loader, loss_fn, metric_fn=metric_fn)\n",
    "        val_loss_epoch.append(vloss)\n",
    "        val_metric_epoch.append(vmetric)\n",
    "\n",
    "        msg = f\"Epoch {ep:02d} | train_loss {train_loss_epoch[-1]:.4f} | val_loss {vloss:.4f}\"\n",
    "        if metric_fn is not None and vmetric is not None:\n",
    "            msg += f\" | val_metric {vmetric:.4f}\"\n",
    "        print(msg)\n",
    "\n",
    "    return {\n",
    "        \"train_loss_step\": train_loss_step,\n",
    "        \"train_loss_epoch\": train_loss_epoch,\n",
    "        \"val_loss_epoch\": val_loss_epoch,\n",
    "        \"val_metric_epoch\": val_metric_epoch,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12f45f",
   "metadata": {},
   "source": [
    "# Part 2 — Implement a 3-Layer MLP for Regression (20 pts)\n",
    "\n",
    "### Model\n",
    "- Two hidden layers + ReLU\n",
    "- Output layer is linear (for regression)\n",
    "\n",
    "### Tasks\n",
    "1. Implement `init_params_mlp(d, h1, h2)` (returns a dict of tensors with `requires_grad=True`).\n",
    "2. Implement `relu(x)`.\n",
    "3. Implement `MLP3` forward.\n",
    "4. Implement MSE loss from scratch.\n",
    "5. Implement SGD updater from scratch.\n",
    "\n",
    "### Report\n",
    "- Plot per-step training loss for **Linear Model vs MLP3** on the nonlinear regression dataset.\n",
    "- Explain (2–4 sentences) why MLP improves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 1 TODO =====\n",
    "\n",
    "def relu(x: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: implement ReLU without nn.ReLU\n",
    "    raise NotImplementedError\n",
    "\n",
    "def mse_loss(yhat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    # TODO: implement mean squared error\n",
    "    raise NotImplementedError\n",
    "\n",
    "def init_params_mlp(d: int, h1: int, h2: int, seed: int = 0, scale: float = 0.01) -> Dict[str, torch.Tensor]:\n",
    "    '''\n",
    "    Baseline init: Normal(0, scale)\n",
    "    Return dict with W1,b1,W2,b2,W3,b3 on correct device and requires_grad=True.\n",
    "    '''\n",
    "    set_seed(seed)\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n",
    "\n",
    "class MLP3:\n",
    "    def __init__(self, d: int, h1: int, h2: int, out_dim: int = 1):\n",
    "        self.params = init_params_mlp(d, h1, h2, seed=0, scale=0.01)\n",
    "        self._train = True\n",
    "\n",
    "    def train_mode(self, is_train: bool = True):\n",
    "        self._train = is_train\n",
    "\n",
    "    def __call__(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        P = self.params\n",
    "        # TODO: implement forward:\n",
    "        # H1 = relu(X @ P[\"W1\"] + P[\"b1\"])\n",
    "        # H2 = relu(H1 @ P[\"W2\"] + P[\"b2\"])\n",
    "        # yhat = H2 @ P[\"W3\"] + P[\"b3\"]\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params.values():\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "def sgd_updater(params: Dict[str, torch.Tensor], lr: float, weight_decay: float = 0.0):\n",
    "    '''\n",
    "    Returns a function updater() that applies SGD to all params.\n",
    "    If weight_decay > 0, include L2 regularization (Part 3A).\n",
    "    '''\n",
    "    def step():\n",
    "        # TODO: implement SGD update: p -= lr * grad (and + weight_decay * p if enabled)\n",
    "        raise NotImplementedError\n",
    "    return step\n",
    "\n",
    "\n",
    "\n",
    "class LinearModel:\n",
    "    def __init__(self, d: int):\n",
    "        self.W = (0.01 * torch.randn(d, 1, device=device)).requires_grad_(True)\n",
    "        self.b = torch.zeros(1, device=device, requires_grad=True)\n",
    "        self._train = True\n",
    "\n",
    "    def train_mode(self, is_train: bool = True):\n",
    "        self._train = is_train\n",
    "\n",
    "    def __call__(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return X @ self.W + self.b\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.W.grad is not None: self.W.grad.zero_()\n",
    "        if self.b.grad is not None: self.b.grad.zero_()\n",
    "\n",
    "\n",
    "def linear_mse(yhat, y):\n",
    "    return ((yhat - y) ** 2).mean()\n",
    "\n",
    "\n",
    "def linear_sgd_updater(model: LinearModel, lr: float):\n",
    "    def step():\n",
    "        with torch.no_grad():\n",
    "            model.W -= lr * model.W.grad\n",
    "            model.b -= lr * model.b.grad\n",
    "    return step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6098fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Regression experiment runner ===\n",
    "# After TODOs are done, run this cell.\n",
    "\n",
    "set_seed(0)\n",
    "d = 3\n",
    "train_loader, val_loader = make_loaders(SyntheticRegression(n=4000, d=d, noise_std=0.1), batch_size=64)\n",
    "\n",
    "lin = LinearModel(d)\n",
    "lin_updater = linear_sgd_updater(lin, lr=0.05)\n",
    "\n",
    "mlp = MLP3(d=d, h1=64, h2=32, out_dim=1)\n",
    "mlp_updater = sgd_updater(mlp.params, lr=0.05)\n",
    "\n",
    "print(\"\\n--- Linear ---\")\n",
    "hist_lin = fit(lin, train_loader, val_loader, loss_fn=linear_mse, updater=lin_updater, epochs=20)\n",
    "\n",
    "print(\"\\n--- MLP3 ---\")\n",
    "hist_mlp = fit(mlp, train_loader, val_loader, loss_fn=mse_loss, updater=mlp_updater, epochs=20)\n",
    "\n",
    "plot_curves({\"linear\": hist_lin[\"train_loss_step\"], \"mlp\": hist_mlp[\"train_loss_step\"]},\n",
    "            title=\"Regression: loss vs step\", ylabel=\"MSE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3012095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46121aba",
   "metadata": {},
   "source": [
    "# Part 3 — Initialization Experiments (20 pts)\n",
    "\n",
    "\n",
    "Compare:\n",
    "- Tiny Gaussian\n",
    "- Xavier\n",
    "- He/Kaiming \n",
    "\n",
    "### Report\n",
    "- Provide a small table (can be markdown) with best validation metric for each setting.\n",
    "- 3–6 sentences: What helped most and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_params_xavier(d: int, h1: int, h2: int, seed: int = 0) -> Dict[str, torch.Tensor]:\n",
    "    # TODO: implement Xavier init for W1,W2,W3; zeros for biases\n",
    "    raise NotImplementedError\n",
    "\n",
    "def init_params_he(d: int, h1: int, h2: int, seed: int = 0) -> Dict[str, torch.Tensor]:\n",
    "    # TODO: implement He/Kaiming init for ReLU layers\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "class MLP3_DropoutInit(MLP3):\n",
    "    def __init__(self, d: int, h1: int, h2: int, out_dim: int = 1, p_drop: float = 0.0, init: str = \"gauss\"):\n",
    "        self._train = True\n",
    "        self.p_drop = p_drop\n",
    "        if init == \"gauss\":\n",
    "            self.params = init_params_mlp(d, h1, h2, seed=0, scale=0.01)\n",
    "        elif init == \"xavier\":\n",
    "            self.params = init_params_xavier(d, h1, h2, seed=0)\n",
    "        elif init == \"he\":\n",
    "            self.params = init_params_he(d, h1, h2, seed=0)\n",
    "        else:\n",
    "            raise ValueError(\"init must be one of: gauss, xavier, he\")\n",
    "\n",
    "    def __call__(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        P = self.params\n",
    "        # TODO: forward with dropout on H1 and/or H2 (training only)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0755300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Part 3 experiment  (fill after TODOs) ===\n",
    "\n",
    "\n",
    "inits = [\"gauss\", \"xavier\", \"he\"]\n",
    "\n",
    "results = []\n",
    "for init in inits:\n",
    "            set_seed(0)\n",
    "            train_loader, val_loader = make_loaders(SyntheticBinaryClassification(n=4000, d=3), batch_size=64)\n",
    "            m = MLP3_Init(d=3, h1=64, h2=32,  init=init)\n",
    "            up = sgd_updater(m.params, lr=0.05)\n",
    "            hist = fit(m, train_loader, val_loader, loss_fn=bce_with_logits_loss, updater=up,\n",
    "                       epochs=10, metric_fn=accuracy_from_logits)\n",
    "            best_acc = max([v for v in hist[\"val_metric_epoch\"] if v is not None])\n",
    "            results.append((wd, p, init, best_acc))\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x[-1], reverse=True)\n",
    "results_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fe782",
   "metadata": {},
   "source": [
    "## Part 3 Report (write here)\n",
    "\n",
    "Create a markdown table like:\n",
    "\n",
    "| weight_decay | dropout_p | init | best_val_acc |\n",
    "|---:|---:|---|---:|\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "Then discuss what helped most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fb584",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a50a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tiny gradient check example (run after your Part 1/2 are working) ===\n",
    "# set_seed(0)\n",
    "# d = 3\n",
    "# X = torch.randn(8, d, device=device)\n",
    "# y = (torch.randn(8, 1, device=device) > 0).float()\n",
    "\n",
    "# m_tiny = MLP3(d=d, h1=4, h2=3, out_dim=1)\n",
    "\n",
    "# result = finite_difference_check(m_tiny, bce_with_logits_loss, X, y, param_name=\"W1\", idx=(0, 0), eps=1e-4)\n",
    "# print(result)\n",
    "\n",
    "# Expected: rel_error ~ 1e-2 or smaller (often much smaller).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fbdd4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
